// Package deepseek provides an unofficial Go client for the DeepSeek API.
// It supports interacting with DeepSeek's chat functionality.
package deepseek

import (
	"context"
	"encoding/json"
	"errors"
	"io"
	"net/http"
	"strconv"

	deepseek "github.com/p9966/go-deepseek/internal"
)

const (
	ChatMessageRoleSystem    = "system"
	ChatMessageRoleUser      = "user"
	ChatMessageRoleAssistant = "assistant"
)

const chatCompletionSuffix = "/chat/completions"

type ChatCompletionRequest struct {
	Model            string                  `json:"model"`
	Messages         []ChatCompletionMessage `json:"messages"`
	FrequencyPenalty float32                 `json:"frequency_penalty"`
	MaxTokens        int                     `json:"max_tokens,omitempty"`       // Optional: Maximum tokens, > 1
	PresencePenalty  float32                 `json:"presence_penalty,omitempty"` // Optional: Presence penalty, >= -2 and <= 2
	Temperature      float32                 `json:"temperature,omitempty"`      // Optional: Sampling temperature, <= 2
	TopP             float32                 `json:"top_p,omitempty"`            // Optional: Nucleus sampling parameter, <= 1
	ResponseFormat   *ResponseFormat         `json:"response_format,omitempty"`  // Optional: Custom response format
	Stop             []string                `json:"stop,omitempty"`             // Optional: Stop signals
	Tools            []Tools                 `json:"tools,omitempty"`            // Optional: List of tools
	LogProbs         bool                    `json:"logprobs,omitempty"`         // Optional: Enable log probabilities
	TopLogProbs      int                     `json:"top_logprobs,omitempty"`     // Optional: Number of top tokens with log probabilities, <= 20
}

type ChatCompletionMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type ResponseFormat struct {
	Type string `json:"type"`
}

type Tools struct {
	Type     string   `json:"type"`
	Function Function `json:"function"`
}

type Function struct {
	Name        string      `json:"name"`                 // The name of the function (required)
	Description string      `json:"description"`          // Description of the function (required)
	Parameters  *Parameters `json:"parameters,omitempty"` // Parameters schema (optional)
}

type Parameters struct {
	Type       string                 `json:"type"` // Type of the parameters, e.g., "object" (required)
	Properties map[string]interface{} `json:"properties,omitempty"`
	Required   []string               `json:"required,omitempty"`
}

type ChatCompletionResponse struct {
	ID                string   `json:"id"`                 // Unique identifier for the chat completion.
	Object            string   `json:"object"`             // Type of the object, typically "chat.completion".
	Created           int64    `json:"created"`            // Timestamp when the chat completion was created.
	Model             string   `json:"model"`              // The model used for generating the completion.
	Choices           []Choice `json:"choices"`            // List of completion choices generated by the model.
	Usage             Usage    `json:"usage"`              // Token usage statistics.
	SystemFingerprint string   `json:"system_fingerprint"` // Fingerprint of the system configuration.
}

type Choice struct {
	Index        int       `json:"index"`              // Index of the choice in the list of choices.
	Message      Message   `json:"message"`            // The message generated by the model.
	LogProbs     *LogProbs `json:"logprobs,omitempty"` // Log probabilities of the tokens, if available.
	FinishReason string    `json:"finish_reason"`      // Reason why the completion finished.
}

type Message struct {
	Role             string `json:"role"`                        // Role of the message sender (e.g., "user", "assistant").
	Content          string `json:"content"`                     // Content of the message.
	ReasoningContent string `json:"reasoning_content,omitempty"` // Optional reasoning content.
}

type LogProbs struct {
	Tokens        []string             `json:"tokens,omitempty"`         // List of tokens.
	TokenLogProbs []float64            `json:"token_logprobs,omitempty"` // Log probabilities of each token.
	TopLogProbs   []map[string]float64 `json:"top_logprobs,omitempty"`   // Top log probabilities for each token.
}

type Usage struct {
	PromptTokens          int `json:"prompt_tokens"`            // Number of tokens used in the prompt.
	CompletionTokens      int `json:"completion_tokens"`        // Number of tokens used in the completion.
	TotalTokens           int `json:"total_tokens"`             // Total number of tokens used.
	PromptCacheHitTokens  int `json:"prompt_cache_hit_tokens"`  // Number of tokens served from cache.
	PromptCacheMissTokens int `json:"prompt_cache_miss_tokens"` // Number of tokens not served from cache.
}

func (c *Client) CreateChatCompletion(ctx context.Context, req *ChatCompletionRequest) (*ChatCompletionResponse, error) {
	if req == nil {
		return nil, errors.New("request can not be nil")
	}

	request, err := deepseek.NewRequestBuilder(c.authToken).SetBaseUrl(c.baseUrl).SetPath(chatCompletionSuffix).SetBody(req).Build(ctx)
	if err != nil {
		return nil, err
	}
	resp, err := c.Do(request)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, errors.New("unexpected status code: " + strconv.Itoa(resp.StatusCode))
	}

	buf, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	var result ChatCompletionResponse
	if err := json.Unmarshal(buf, &result); err != nil {
		return nil, err
	}
	return &result, nil
}
